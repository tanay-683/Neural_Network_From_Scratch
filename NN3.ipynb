{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do ?\n",
    "done with forward pass \n",
    "- implement backward pass `fit method`\n",
    "- multi dimensional input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import mercury as mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = np.random.randint(2, 5, (30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 4, 3, 2, 2, 4, 4, 3, 3, 4,\n",
       "       2, 2, 3, 4, 3, 2, 4, 3])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input:\n",
    "    \n",
    "    '''\n",
    "    making it for only one dimensional input\n",
    "    '''\n",
    "    def __init__(self, input_shape:int, layer_name:str):\n",
    "        \n",
    "        self.layer_name = layer_name\n",
    "        self.units = input_shape\n",
    "        self.input_data:np.array = np.zeros(self.units) # None only at time of initialization\n",
    "        \n",
    "        # these parameters doesnt exist for input layer\n",
    "        self.previous_layer = None\n",
    "        self.previous_layer_units = None\n",
    "        self.__weights = None\n",
    "        self.__biases = None\n",
    "        \n",
    "        \n",
    "    def forward_pass(self):\n",
    "        return self.input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensePhaseThree:\n",
    "    \n",
    "    def __init__(self, units:int, activation:str, layer_name:str):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.layer_name = layer_name\n",
    "        \n",
    "        self.previous_layer:DensePhaseThree = None\n",
    "        self.previous_layer_values = None\n",
    "        self.previous_layer_units = None\n",
    "        self.__weights = None\n",
    "        self.__biases = None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward_pass(self) -> np.array:\n",
    "\n",
    "        #  determining previous layer units\n",
    "        \n",
    "        self.previous_layer_units = self.previous_layer.units\n",
    "        \n",
    "        # if the previous layer is input layer then previous layer values will be input data\n",
    "        if isinstance(self.previous_layer, Input):\n",
    "            self.previous_layer_values = self.previous_layer.input_data\n",
    "\n",
    "        # if the previous layer is dense layer then previous layer values will be activation output of previous layer\n",
    "        if isinstance(self.previous_layer, DensePhaseThree):\n",
    "            self.previous_layer_values = self.previous_layer.activation_output\n",
    "\n",
    "                \n",
    "        # initializing weights and biases of this layer\n",
    "        self.__weights, self.__biases = self.__initialize_weights_and_biases(self.previous_layer_units)\n",
    "        \n",
    "        \n",
    "        # calculating output ---> WiXi + Bo \n",
    "        self.output = np.add(np.dot(self.__weights, self.previous_layer_values), self.__biases)\n",
    "\n",
    "\n",
    "        # applying activation function on output\n",
    "        if self.activation.lower() == 'relu':\n",
    "            activation_func = np.vectorize(self.__relu_activation)\n",
    "            self.activation_output = activation_func(self.output).astype(np.float32)\n",
    "            \n",
    "        elif self.activation.lower() == 'sigmoid':\n",
    "            activation_func = np.vectorize(self.__sigmoid_activation)\n",
    "            self.activation_output = activation_func(self.output).astype(np.float32)\n",
    "            \n",
    "        else: # linear\n",
    "            self.activation_output = self.output\n",
    "            \n",
    "        print(\"============ Layer Name: \", self.layer_name.upper(), \" ============\")\n",
    "        print(\"\\n\\ncurrent_layer_units = \", self.units, \"\\n\")\n",
    "        print(\"biases_shape = \", self.__biases.shape, \"\\n\")\n",
    "        print(\"weights_shape = \", self.__weights.shape, \"\\n\")\n",
    "        print(\"previous layer units = \", self.previous_layer_units, \"\\n\")\n",
    "        print(\"previous layer values = \", self.previous_layer_values, \"\\n\")\n",
    "        print(\"Output for next layer is =\", self.activation_output, \"\\n\")\n",
    "        print(\"===============================================\")\n",
    "        \n",
    "\n",
    "        \n",
    "        return self.activation_output\n",
    "\n",
    "        \n",
    "    def __initialize_weights_and_biases(self,previous_layer_units:int):\n",
    "        \n",
    "        '''\n",
    "        Note: using `he normal` initialization\n",
    "        '''\n",
    "        \n",
    "        biases = np.random.randn(self.units)*np.sqrt(2/previous_layer_units)\n",
    "        weights = np.random.randn(self.units, previous_layer_units)*np.sqrt(2/previous_layer_units)\n",
    "        \n",
    "        return weights, biases\n",
    "        \n",
    "\n",
    "    def __relu_activation(self, layer_output:np.array):\n",
    "        return max(0, layer_output)\n",
    "    \n",
    "    \n",
    "    def __sigmoid_activation(self, layer_output:np.array):\n",
    "        if layer_output>0:\n",
    "            return 1/(1+np.exp(-layer_output)) # 1/(1+e^-z)\n",
    "        else:\n",
    "            return np.exp(layer_output)/(1+np.exp(layer_output)) # e^z / (1+e^z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.count = 1\n",
    "        \n",
    "        \n",
    "               \n",
    "    def add(self, layer:DensePhaseThree):\n",
    "        layer.previous_layer = None\n",
    "        '''\n",
    "        set last item in list as previous layer\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def show_layers(self):\n",
    "        print(\"=================== LAYER INFO ===================\")\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f\"\"\"\n",
    "                  Index::{index}\n",
    "                        Layer Name:: {layer.layer_name} {type(layer.layer_name)}\n",
    "                        Layer Address:: {layer}  {type(layer)}      \n",
    "                        Layer units::{layer.units} {type(layer.units)}\n",
    "                        Previous Layer::{layer.previous_layer} {type(layer.previous_layer)}\n",
    "                        \n",
    "                        \"\"\")\n",
    "                \n",
    "            \n",
    "    def compile_(self, input_data:np.array): \n",
    "        '''\n",
    "        connecting all the layers\n",
    "        \n",
    "        running forward pass for all layers\n",
    "        \n",
    "        and connect input layer with input data\n",
    "        '''       \n",
    "        \n",
    "        #  connecting all the layers\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            # print(f\"set {self.layers[i].layer_name}::{self.layers[i]} as previous layer of {self.layers[i+1].layer_name}::{self.layers[i+1]}\")\n",
    "            self.layers[i+1].previous_layer = self.layers[i]\n",
    "\n",
    "        # print(\"\\n\\nDONE WITH CONNECTING ALL THE LAYERS \\n\\n\")\n",
    "        \n",
    "        if isinstance(self.layers[0], Input) and self.layers[0].units == input_data.shape[0]:\n",
    "            self.layers[0].input_data = input_data\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            layer.forward_pass() \n",
    "            \n",
    "            \n",
    "    def loss_calc(self, y_true:np.array, y_pred:np.array):\n",
    "        '''\n",
    "        calculating mean squared error\n",
    "        '''\n",
    "        return np.mean(np.square(y_true - y_pred))   \n",
    "            \n",
    "    def fit(self):\n",
    "        '''\n",
    "        initialize weights and biases with 0 & 1,\n",
    "        take weights and add 100 to it, \n",
    "        do it 5 times(epochs) and\n",
    "        dont update  weights\n",
    "        # '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Input):\n",
    "                print(\"no weights and biases update for input layer\\n\")\n",
    "                continue\n",
    "            \n",
    "            print(\"weights before updating = \", layer._DensePhaseThree__weights, \"\\n\")\n",
    "            layer._DensePhaseThree__weights = layer._DensePhaseThree__weights * 100\n",
    "            print(\"weights after updating = \", layer._DensePhaseThree__weights, \"\\n\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            print(\"biases before updating = \", layer._DensePhaseThree__biases, \"\\n\")\n",
    "            layer._DensePhaseThree__biases = layer._DensePhaseThree__biases + 200\n",
    "            print(\"biases after updating = \", layer._DensePhaseThree__biases, \"\\n\")\n",
    "            print(\"\\n==============================\\n\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Input(input_shape=input_.shape[0], layer_name=\"input_\"))\n",
    "model.add(DensePhaseThree(units = 1024,activation = \"relu\", layer_name=\"first\"))\n",
    "model.add(DensePhaseThree(units = 512,activation = \"relu\", layer_name=\"second\"))\n",
    "model.add(DensePhaseThree(units = 2,activation = \"SIGMOID\", layer_name=\"third\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Layer Name:  FIRST  ============\n",
      "\n",
      "\n",
      "current_layer_units =  1024 \n",
      "\n",
      "biases_shape =  (1024,) \n",
      "\n",
      "weights_shape =  (1024, 30) \n",
      "\n",
      "previous layer units =  30 \n",
      "\n",
      "previous layer values =  [3 2 2 3 3 3 2 2 3 3 3 3 3 4 3 2 2 4 4 3 3 4 2 2 3 4 3 2 4 3] \n",
      "\n",
      "Output for next layer is = [2.940445  0.        5.4812737 ... 0.        0.        0.       ] \n",
      "\n",
      "===============================================\n",
      "============ Layer Name:  SECOND  ============\n",
      "\n",
      "\n",
      "current_layer_units =  512 \n",
      "\n",
      "biases_shape =  (512,) \n",
      "\n",
      "weights_shape =  (512, 1024) \n",
      "\n",
      "previous layer units =  1024 \n",
      "\n",
      "previous layer values =  [2.940445  0.        5.4812737 ... 0.        0.        0.       ] \n",
      "\n",
      "Output for next layer is = [ 0.71260387  0.          5.0316224   7.2424097   0.          1.3005242\n",
      "  0.          0.          0.          0.24294141  0.          2.3072357\n",
      "  0.46790725  6.894556    0.          0.81726557  3.2096462   2.1931748\n",
      "  0.          6.215185    4.165792    0.          0.          0.\n",
      "  0.          3.1342254   0.5504566   0.          0.9520564   0.\n",
      "  0.          0.          0.          0.          4.177888    0.\n",
      "  0.36991212  1.7192062   7.0534816   0.          0.          0.64154124\n",
      "  5.9918523   0.          2.126237    0.          0.          0.23088364\n",
      "  5.8288794   0.47483796  4.5047636   0.          0.18746163  4.0729704\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.4405005   0.          0.          0.          0.\n",
      "  0.          0.          1.5840834   0.          0.          3.362874\n",
      "  2.9986503   0.          0.          0.          0.43425822  0.66080225\n",
      "  4.1866045   4.9758725   0.          0.5024706   0.          0.\n",
      "  0.          1.4121332   0.          4.452681    0.          0.\n",
      "  0.          0.          1.3539243   0.          0.33471274  0.\n",
      "  0.          7.6883316   0.          0.          0.          0.\n",
      "  0.          0.          6.512615    0.          0.          0.\n",
      "  0.          0.          4.761409    1.3125749   0.42835817  5.0876007\n",
      "  0.          2.5330284   0.         15.519055    0.          0.\n",
      "  0.6628424   5.216704    0.          8.014324    0.          5.725289\n",
      "  0.          0.          4.495748    0.          0.          6.399723\n",
      "  0.          0.83844334  1.2457664   0.          0.5252493   0.\n",
      "  2.090257    0.          0.          4.4381766   1.4576817   9.311231\n",
      "  0.          0.          0.          0.          0.          0.67524487\n",
      "  5.085778    0.          0.5174648   1.6677032   6.3111634   0.\n",
      "  5.1808124   0.          1.5464916   0.          0.          7.345394\n",
      "  0.47989675  2.2851746   0.6315421   2.3997521   0.          0.\n",
      "  0.          2.3632646   1.0736115   4.957513    0.68396753  0.\n",
      "  0.1855846   0.          0.          2.5821292   0.          2.671659\n",
      "  0.6582985   8.328448    0.          0.          5.432358    1.340179\n",
      "  4.3501806   0.          0.          1.9306077  11.059686    0.\n",
      "  0.          3.4605606   0.          0.          1.542217    0.\n",
      "  2.0649374   8.099039    0.          0.          1.6422507   1.2198483\n",
      "  4.7066684   3.01979     0.          0.8764777   5.7188816   1.8306971\n",
      "  0.84592026  1.6426303   0.          0.          0.          0.\n",
      "  0.          1.8876624   3.0757537   2.85366     0.18705806  3.5726666\n",
      "  0.          0.          0.          0.          3.646799    0.\n",
      "  0.          4.038315    2.3581243   0.          0.9394256   0.\n",
      "  0.          0.          0.          0.          1.2009748   2.3550603\n",
      "  4.344016    6.8193774   5.447864    0.          1.8235255   0.\n",
      "  2.9753027   2.0351646   0.          0.          0.          4.5059943\n",
      "  0.          0.          5.7646236   6.1014824   1.8394542   0.\n",
      "  0.          4.547859    0.          0.          2.9779754   0.\n",
      "  0.          3.518642   13.868146    0.          0.34340838  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.30423197  5.272529    5.921149    3.6007679\n",
      "  0.          2.1587846   0.          0.          0.          1.2232394\n",
      "  0.          3.419266    0.          0.          0.          0.\n",
      "  3.4405253   6.912996    5.1449704   0.          1.0435699   0.\n",
      "  0.11630336  0.          0.          0.         11.332827    0.\n",
      "  0.          4.5015273   0.          0.          1.2971154   0.\n",
      "  0.9731359   0.          1.0619135   2.5839055   0.69664574  0.\n",
      "  0.0471131   3.7815065   4.024499    0.          0.          2.5930705\n",
      "  0.          0.          0.          4.158659    0.          2.9284334\n",
      "  1.0270097   1.2052159   1.0031856   2.6557066   0.24945642  0.\n",
      "  3.4162474   1.2171773   0.          0.          0.          0.\n",
      "  0.          0.16435842  1.5426172   3.3572736   0.86061275  0.\n",
      "  3.5005991   7.244515    0.          8.532045    0.          5.4592214\n",
      "  7.283937    3.2990472   0.          0.          0.          0.\n",
      "  0.12088156  4.383213    5.692007    0.          3.4868636   1.3569446\n",
      "  5.8781      4.2766147   5.9450674   3.340697    2.087483    1.5207795\n",
      "  0.          0.          0.          0.          0.          4.65584\n",
      "  0.          3.6465356   5.195546    0.          0.          1.1856759\n",
      "  2.1519327   7.249652    0.          0.615852    0.          1.9148636\n",
      "  0.          0.          0.          5.5763936   4.100974    0.81004053\n",
      "  0.4173952   1.9320457   0.          3.953788    0.         10.50482\n",
      "  0.          3.2901118   0.2775876   0.          2.7394736   0.\n",
      "  0.          0.          3.9963439   1.5920217   2.1574447   0.\n",
      "  0.          0.          0.          0.3455014   0.          3.0656643\n",
      "  0.          0.          0.          0.          2.854215    0.7804509\n",
      "  1.583206    0.          1.6583703   6.2794156   3.9708314   6.1369076\n",
      "  0.          3.4667785   0.93949735  0.          0.          7.8519955\n",
      "  0.          0.          0.11905848  0.81801265  1.6524351   3.687937\n",
      "  0.          0.5851118   3.1657321   0.9554381   0.          0.6577264\n",
      "  0.          0.          0.65683377  4.4659925   0.          5.604798\n",
      "  0.          0.          2.7047184   1.2014911   1.8482649   0.\n",
      "  0.          0.          6.371245    0.6207697   0.          1.7356932\n",
      "  0.          0.78934306  0.          0.          0.          0.4889154\n",
      "  0.          0.          0.49694487  0.          0.          2.9878998\n",
      "  0.          1.9153401   0.          0.          2.4481854   2.2331154\n",
      "  0.          0.          2.1512327   2.4163141   0.67644244  0.\n",
      "  0.          4.35974     0.6710902   3.127656    0.          8.72718\n",
      "  0.          8.612425    0.          0.10861862  0.          0.6737742\n",
      "  3.1778767   0.11726025  7.9891996   6.2803583   0.2260133   0.\n",
      "  5.007737    0.        ] \n",
      "\n",
      "===============================================\n",
      "============ Layer Name:  THIRD  ============\n",
      "\n",
      "\n",
      "current_layer_units =  2 \n",
      "\n",
      "biases_shape =  (2,) \n",
      "\n",
      "weights_shape =  (2, 512) \n",
      "\n",
      "previous layer units =  512 \n",
      "\n",
      "previous layer values =  [ 0.71260387  0.          5.0316224   7.2424097   0.          1.3005242\n",
      "  0.          0.          0.          0.24294141  0.          2.3072357\n",
      "  0.46790725  6.894556    0.          0.81726557  3.2096462   2.1931748\n",
      "  0.          6.215185    4.165792    0.          0.          0.\n",
      "  0.          3.1342254   0.5504566   0.          0.9520564   0.\n",
      "  0.          0.          0.          0.          4.177888    0.\n",
      "  0.36991212  1.7192062   7.0534816   0.          0.          0.64154124\n",
      "  5.9918523   0.          2.126237    0.          0.          0.23088364\n",
      "  5.8288794   0.47483796  4.5047636   0.          0.18746163  4.0729704\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.4405005   0.          0.          0.          0.\n",
      "  0.          0.          1.5840834   0.          0.          3.362874\n",
      "  2.9986503   0.          0.          0.          0.43425822  0.66080225\n",
      "  4.1866045   4.9758725   0.          0.5024706   0.          0.\n",
      "  0.          1.4121332   0.          4.452681    0.          0.\n",
      "  0.          0.          1.3539243   0.          0.33471274  0.\n",
      "  0.          7.6883316   0.          0.          0.          0.\n",
      "  0.          0.          6.512615    0.          0.          0.\n",
      "  0.          0.          4.761409    1.3125749   0.42835817  5.0876007\n",
      "  0.          2.5330284   0.         15.519055    0.          0.\n",
      "  0.6628424   5.216704    0.          8.014324    0.          5.725289\n",
      "  0.          0.          4.495748    0.          0.          6.399723\n",
      "  0.          0.83844334  1.2457664   0.          0.5252493   0.\n",
      "  2.090257    0.          0.          4.4381766   1.4576817   9.311231\n",
      "  0.          0.          0.          0.          0.          0.67524487\n",
      "  5.085778    0.          0.5174648   1.6677032   6.3111634   0.\n",
      "  5.1808124   0.          1.5464916   0.          0.          7.345394\n",
      "  0.47989675  2.2851746   0.6315421   2.3997521   0.          0.\n",
      "  0.          2.3632646   1.0736115   4.957513    0.68396753  0.\n",
      "  0.1855846   0.          0.          2.5821292   0.          2.671659\n",
      "  0.6582985   8.328448    0.          0.          5.432358    1.340179\n",
      "  4.3501806   0.          0.          1.9306077  11.059686    0.\n",
      "  0.          3.4605606   0.          0.          1.542217    0.\n",
      "  2.0649374   8.099039    0.          0.          1.6422507   1.2198483\n",
      "  4.7066684   3.01979     0.          0.8764777   5.7188816   1.8306971\n",
      "  0.84592026  1.6426303   0.          0.          0.          0.\n",
      "  0.          1.8876624   3.0757537   2.85366     0.18705806  3.5726666\n",
      "  0.          0.          0.          0.          3.646799    0.\n",
      "  0.          4.038315    2.3581243   0.          0.9394256   0.\n",
      "  0.          0.          0.          0.          1.2009748   2.3550603\n",
      "  4.344016    6.8193774   5.447864    0.          1.8235255   0.\n",
      "  2.9753027   2.0351646   0.          0.          0.          4.5059943\n",
      "  0.          0.          5.7646236   6.1014824   1.8394542   0.\n",
      "  0.          4.547859    0.          0.          2.9779754   0.\n",
      "  0.          3.518642   13.868146    0.          0.34340838  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.30423197  5.272529    5.921149    3.6007679\n",
      "  0.          2.1587846   0.          0.          0.          1.2232394\n",
      "  0.          3.419266    0.          0.          0.          0.\n",
      "  3.4405253   6.912996    5.1449704   0.          1.0435699   0.\n",
      "  0.11630336  0.          0.          0.         11.332827    0.\n",
      "  0.          4.5015273   0.          0.          1.2971154   0.\n",
      "  0.9731359   0.          1.0619135   2.5839055   0.69664574  0.\n",
      "  0.0471131   3.7815065   4.024499    0.          0.          2.5930705\n",
      "  0.          0.          0.          4.158659    0.          2.9284334\n",
      "  1.0270097   1.2052159   1.0031856   2.6557066   0.24945642  0.\n",
      "  3.4162474   1.2171773   0.          0.          0.          0.\n",
      "  0.          0.16435842  1.5426172   3.3572736   0.86061275  0.\n",
      "  3.5005991   7.244515    0.          8.532045    0.          5.4592214\n",
      "  7.283937    3.2990472   0.          0.          0.          0.\n",
      "  0.12088156  4.383213    5.692007    0.          3.4868636   1.3569446\n",
      "  5.8781      4.2766147   5.9450674   3.340697    2.087483    1.5207795\n",
      "  0.          0.          0.          0.          0.          4.65584\n",
      "  0.          3.6465356   5.195546    0.          0.          1.1856759\n",
      "  2.1519327   7.249652    0.          0.615852    0.          1.9148636\n",
      "  0.          0.          0.          5.5763936   4.100974    0.81004053\n",
      "  0.4173952   1.9320457   0.          3.953788    0.         10.50482\n",
      "  0.          3.2901118   0.2775876   0.          2.7394736   0.\n",
      "  0.          0.          3.9963439   1.5920217   2.1574447   0.\n",
      "  0.          0.          0.          0.3455014   0.          3.0656643\n",
      "  0.          0.          0.          0.          2.854215    0.7804509\n",
      "  1.583206    0.          1.6583703   6.2794156   3.9708314   6.1369076\n",
      "  0.          3.4667785   0.93949735  0.          0.          7.8519955\n",
      "  0.          0.          0.11905848  0.81801265  1.6524351   3.687937\n",
      "  0.          0.5851118   3.1657321   0.9554381   0.          0.6577264\n",
      "  0.          0.          0.65683377  4.4659925   0.          5.604798\n",
      "  0.          0.          2.7047184   1.2014911   1.8482649   0.\n",
      "  0.          0.          6.371245    0.6207697   0.          1.7356932\n",
      "  0.          0.78934306  0.          0.          0.          0.4889154\n",
      "  0.          0.          0.49694487  0.          0.          2.9878998\n",
      "  0.          1.9153401   0.          0.          2.4481854   2.2331154\n",
      "  0.          0.          2.1512327   2.4163141   0.67644244  0.\n",
      "  0.          4.35974     0.6710902   3.127656    0.          8.72718\n",
      "  0.          8.612425    0.          0.10861862  0.          0.6737742\n",
      "  3.1778767   0.11726025  7.9891996   6.2803583   0.2260133   0.\n",
      "  5.007737    0.        ] \n",
      "\n",
      "Output for next layer is = [0.54790515 0.5243255 ] \n",
      "\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "model.compile_(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights before updating =  [[-0.03038823 -0.02773302 -0.00575415 ...  0.00771625 -0.01516745\n",
      "  -0.09889268]\n",
      " [-0.06693592  0.01793943 -0.08872091 ... -0.0769863   0.06780039\n",
      "   0.02913393]] \n",
      "\n",
      "weights after updating =  [[-3.0388232  -2.77330225 -0.57541508 ...  0.77162467 -1.51674508\n",
      "  -9.88926784]\n",
      " [-6.69359165  1.7939432  -8.87209067 ... -7.69862959  6.7800391\n",
      "   2.91339276]] \n",
      "\n",
      "biases before updating =  [ 0.05613689 -0.0851112 ] \n",
      "\n",
      "biases after updating =  [200.05613689 199.9148888 ] \n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "weights before updating =  [[-0.03839465 -0.02182454  0.0229833  ...  0.05208794  0.00650265\n",
      "  -0.02857889]\n",
      " [ 0.00536811  0.02777356  0.04865701 ...  0.01689733 -0.07412545\n",
      "  -0.00363756]\n",
      " [ 0.02625414  0.11490749  0.04910597 ... -0.05201946  0.00996389\n",
      "  -0.00360895]\n",
      " ...\n",
      " [ 0.08129636 -0.0019764   0.06947567 ... -0.0229606   0.00867594\n",
      "   0.00577343]\n",
      " [ 0.00089338 -0.00518507  0.05173811 ...  0.03374507 -0.04190707\n",
      "   0.05553855]\n",
      " [ 0.00186204  0.03127229  0.01459276 ...  0.06932744 -0.01573538\n",
      "  -0.04429246]] \n",
      "\n",
      "weights after updating =  [[-3.83946544 -2.18245432  2.29832985 ...  5.20879361  0.65026463\n",
      "  -2.85788926]\n",
      " [ 0.53681107  2.77735559  4.86570135 ...  1.68973262 -7.4125455\n",
      "  -0.36375645]\n",
      " [ 2.62541353 11.49074911  4.91059729 ... -5.20194594  0.99638936\n",
      "  -0.36089475]\n",
      " ...\n",
      " [ 8.12963604 -0.19764001  6.94756683 ... -2.29606022  0.86759417\n",
      "   0.5773432 ]\n",
      " [ 0.08933769 -0.51850662  5.17381111 ...  3.37450698 -4.19070701\n",
      "   5.55385459]\n",
      " [ 0.18620448  3.12722863  1.45927601 ...  6.93274416 -1.57353799\n",
      "  -4.42924579]] \n",
      "\n",
      "biases before updating =  [-1.24029091e-02  4.90363123e-02  9.50665113e-03 -1.05403311e-02\n",
      " -4.04036749e-03  1.45191575e-02  8.03996939e-03 -3.13404099e-02\n",
      " -3.08933918e-02 -2.31884138e-02 -3.09849343e-02 -1.68515307e-02\n",
      "  3.19073510e-02 -7.36762029e-02 -3.26632936e-02  3.87628488e-02\n",
      "  3.68502989e-02 -9.34082340e-04 -5.41543690e-02  6.33845763e-02\n",
      "  2.64995945e-02 -6.57212129e-02  1.18570501e-01 -5.04603963e-02\n",
      "  7.78114807e-03 -6.35648227e-02 -5.89148026e-02  2.08933064e-02\n",
      " -4.39036398e-02  7.36365073e-02  3.00665559e-02 -5.57885210e-02\n",
      "  1.18840547e-02  3.63342814e-02 -7.36924265e-02  3.89610779e-02\n",
      "  4.27494033e-02 -1.67549365e-02 -1.35096239e-03  2.73441813e-02\n",
      " -5.11074219e-02  2.04118825e-02 -3.24933801e-02  1.07161592e-01\n",
      " -3.22282124e-02  4.84522238e-02 -1.45779275e-02 -5.50061642e-02\n",
      "  5.44383791e-02 -4.33398429e-02 -5.14497609e-02 -6.72840606e-03\n",
      " -7.12417288e-03  5.53190656e-02  1.90705205e-02  1.03594540e-01\n",
      " -1.05513628e-02  3.38770160e-02 -3.55236585e-02  1.23008098e-01\n",
      " -4.34746314e-02  2.98238472e-02  1.94546618e-02 -1.82030193e-02\n",
      "  4.75095821e-03  3.53313485e-03  4.84730343e-02  2.52746928e-02\n",
      " -7.45919684e-02 -1.07996732e-01 -6.94015419e-03 -5.22996087e-02\n",
      "  1.84387747e-03 -6.42218666e-03 -5.41956815e-02 -6.65776202e-02\n",
      "  7.93179635e-03 -1.79062045e-03 -2.03065039e-02  2.40096122e-02\n",
      " -8.88738173e-03  2.32710428e-02  3.28443384e-02 -1.79220847e-02\n",
      "  2.94523345e-02 -2.72170710e-02  4.29717177e-02  1.73344876e-02\n",
      " -6.18110433e-02  1.11013830e-03 -7.21661850e-02  6.62816444e-03\n",
      " -6.02360030e-03 -9.00972496e-02  2.51779376e-02  2.39969080e-02\n",
      "  1.22306213e-02 -4.70619225e-02 -6.46162187e-02  1.29474521e-02\n",
      "  1.39334607e-02 -6.49807780e-02  4.84432103e-02  1.83455644e-02\n",
      "  1.48091051e-02 -6.69060358e-02  2.15541838e-03  7.15622788e-03\n",
      " -7.47266551e-02 -7.77682388e-02  5.71987853e-02  5.00117563e-02\n",
      " -2.13703644e-02 -4.98025862e-02  1.66015937e-02 -5.50137378e-02\n",
      "  4.02054154e-02 -5.96019313e-03 -4.25287336e-02 -4.26881101e-02\n",
      "  5.14312063e-02  2.68195757e-02 -3.62789437e-02  3.07710479e-03\n",
      " -4.94199576e-02 -3.00369728e-02 -3.68454434e-02 -1.11719936e-02\n",
      "  5.01582158e-02 -6.48537612e-02  1.75742738e-02 -4.23467039e-02\n",
      "  6.56540029e-03 -1.28914801e-02  1.35536849e-01 -3.96137415e-03\n",
      " -7.54941037e-02  5.73803179e-02 -4.98294252e-02 -2.99533745e-02\n",
      " -1.15369614e-02  1.65108301e-02 -1.41187346e-02  2.10572384e-03\n",
      " -5.64613076e-02 -9.86760251e-03  4.10798019e-02  6.08944966e-03\n",
      " -7.54159178e-03  3.79186149e-02 -1.66070292e-02 -4.74056873e-02\n",
      " -5.10544418e-03 -5.16269787e-02 -1.41797422e-02 -1.59030945e-03\n",
      "  2.41842966e-02  1.59679959e-02 -1.00005918e-02  1.35385749e-02\n",
      "  5.90029235e-02 -8.65180731e-02  5.02151287e-02  5.28663583e-02\n",
      " -2.70204347e-02 -1.72334762e-02 -3.48583246e-02  4.68984275e-02\n",
      " -1.92565066e-03  5.67201004e-02  2.54092026e-02 -1.62391383e-02\n",
      "  4.91538509e-02  1.97014969e-02 -8.03855754e-02  4.67286172e-02\n",
      "  6.08444455e-02  8.40164053e-03 -1.54777856e-02  8.74992104e-02\n",
      " -4.72079303e-02  4.64713148e-02 -2.54944423e-02  3.09721065e-02\n",
      "  5.44891872e-02  2.67633813e-02 -7.69038745e-02 -2.92781441e-02\n",
      " -4.57927184e-02  9.16246366e-03 -1.52483554e-02 -3.90834238e-02\n",
      "  3.72913160e-02 -9.59025291e-02  6.63474813e-02 -8.25709429e-02\n",
      "  1.62233019e-02 -2.09594864e-04  6.54410050e-02 -3.50473270e-02\n",
      " -3.50297040e-02 -4.59288867e-02 -2.30304122e-02 -5.48608701e-02\n",
      "  1.84837398e-02 -1.50310695e-02  2.06967636e-02 -5.06795999e-03\n",
      " -5.95526019e-06  6.30862399e-02  2.82983444e-02  1.12357605e-02\n",
      " -4.30011920e-02 -1.13562520e-03  1.59508604e-02  4.30321151e-02\n",
      "  2.46160759e-02 -4.63584877e-03  1.02936160e-02 -2.48170241e-02\n",
      "  6.06809848e-02 -1.02309218e-02  1.11296670e-01 -4.04403617e-02\n",
      " -1.79403960e-02  5.08759128e-02  4.37192908e-02 -3.98539970e-02\n",
      " -2.90250081e-02 -1.79591832e-02 -1.24023715e-02 -3.59362700e-02\n",
      " -5.42707626e-02  2.50174899e-02 -1.29920769e-02 -5.98469384e-02\n",
      "  8.99410733e-03  1.57538231e-02 -1.06588563e-01 -2.23414335e-02\n",
      " -2.30578796e-02  1.08932089e-02  5.01955224e-02 -2.78095567e-02\n",
      " -1.51276221e-02 -8.32064765e-06  1.44442988e-01 -5.22109221e-02\n",
      " -9.55017948e-02  7.00787147e-02 -9.54682451e-02  2.74321204e-02\n",
      "  5.88523050e-02  1.41064135e-02  1.22844637e-03  2.26419804e-02\n",
      " -8.77717159e-02 -2.32486354e-02  2.55478911e-02 -2.44939098e-02\n",
      " -6.06445394e-02  1.01449785e-01 -2.11168627e-02 -6.02473733e-02\n",
      "  2.69290177e-02  2.17833027e-03 -2.53701746e-02  2.68958245e-02\n",
      " -1.72224075e-02 -8.93995781e-03  2.52060188e-03 -2.29425971e-03\n",
      " -3.53146831e-02  2.10798053e-02 -2.08023704e-02  2.21101827e-02\n",
      "  3.14722427e-02 -3.94926721e-02  9.38819295e-02  1.37898388e-02\n",
      " -5.90809929e-03 -1.60170671e-02 -4.05171081e-02  5.01604351e-02\n",
      " -4.56127413e-02 -3.71033760e-03  2.76378370e-02 -6.34400422e-02\n",
      "  9.31136664e-02  6.37202401e-02  1.59049602e-03 -5.39404003e-02\n",
      " -4.45492503e-02 -1.94508477e-02 -2.31061987e-02 -3.56782530e-02\n",
      "  4.45201770e-02 -6.86443218e-02  7.18254579e-02 -1.13918675e-02\n",
      "  4.03313899e-02  1.02964303e-02  1.77029186e-02 -2.13111595e-03\n",
      "  1.17148580e-01 -1.35264459e-02 -3.08145720e-02 -8.46315692e-02\n",
      " -1.83101110e-03 -6.36871399e-02 -8.07587549e-02 -2.58258319e-02\n",
      " -4.89834258e-02  2.76676097e-02  4.24180232e-02 -1.13094272e-01\n",
      "  1.61666276e-02 -1.22400440e-02 -5.78568901e-02  2.31746798e-02\n",
      "  3.74208299e-02  2.12399572e-02  5.59616752e-04 -9.18002585e-02\n",
      "  5.39311936e-02  7.65330901e-02  7.80828747e-02 -5.44188200e-02\n",
      " -8.76114264e-02 -4.10545673e-02  1.76542350e-03  9.19110618e-03\n",
      " -5.33745379e-03  1.98756290e-02  4.45891615e-02  1.87253904e-03\n",
      " -3.50695325e-03  6.01856701e-02 -6.89655284e-03  5.04518195e-02\n",
      "  4.92864181e-02 -7.18213043e-02 -2.81635559e-02  9.08466915e-04\n",
      "  2.65132872e-03  6.44630471e-02  2.45200037e-02  1.82010252e-02\n",
      " -3.96218876e-02 -4.91908121e-02 -6.30096794e-02  5.01090579e-02\n",
      "  3.62788031e-02  7.55420700e-02 -4.16959583e-02  8.94978097e-02\n",
      " -9.41901287e-02  2.96535364e-02 -1.41738135e-02  1.09484818e-01\n",
      "  3.52044574e-02 -1.94507286e-02 -2.47520566e-02  1.00818073e-01\n",
      "  1.98286534e-02  3.25597076e-02 -8.90966776e-03 -1.29509852e-02\n",
      "  1.75890567e-02 -2.06833348e-02  1.83130329e-02  3.26498139e-02\n",
      " -1.20589941e-02 -1.79956695e-03 -5.44142225e-02 -3.33828097e-02\n",
      "  2.09900440e-02  1.12026728e-02 -3.15224571e-02 -3.67751349e-02\n",
      " -5.77188687e-02 -1.09940619e-02  7.06667209e-02 -1.35447222e-02\n",
      "  1.08815016e-02 -3.92903439e-02  4.63490202e-02 -7.98882460e-02\n",
      " -2.38037595e-03  3.36003762e-02  1.25097731e-02  3.36284253e-02\n",
      "  9.84483907e-02  3.24745909e-02 -5.03712314e-02 -1.36374074e-02\n",
      " -1.44160184e-02  5.51857731e-02 -4.32247872e-02 -9.61995369e-02\n",
      "  2.08153726e-02 -3.57409339e-02 -2.90285730e-02  3.62398340e-02\n",
      " -7.42940537e-02 -4.76356512e-02 -3.97619413e-02  4.16506473e-02\n",
      "  3.12937751e-03 -4.43618069e-02 -2.27642833e-02  4.32623677e-03\n",
      " -1.64404025e-02 -6.37213366e-02  1.70959778e-02  8.65660520e-02\n",
      "  2.67731966e-02 -7.34521124e-03  1.80369438e-02  4.25099039e-02\n",
      " -7.77774804e-02  8.41038994e-03 -2.03944864e-02  2.28761053e-02\n",
      "  2.39535749e-02 -3.56231103e-02 -3.77140101e-02 -3.27582038e-02\n",
      " -2.85255465e-02 -6.84852820e-02 -2.63455473e-02  3.98639821e-02\n",
      " -7.33476879e-02 -1.54486550e-02 -7.64916321e-03  1.12239738e-02\n",
      " -3.65874815e-02 -3.60324048e-02  6.80211266e-02  2.42765108e-02\n",
      " -8.46605023e-03  1.46432994e-04 -8.33887006e-03 -2.07295445e-02\n",
      " -8.23039469e-02  4.46439625e-03  4.73363387e-02  5.56542162e-02\n",
      "  4.08750622e-02  2.42588300e-02  8.82522333e-02 -6.12809471e-02\n",
      " -2.57733942e-02  2.30288636e-02 -3.23507904e-02  1.90517904e-02\n",
      "  5.12602619e-02 -7.47155426e-02 -2.46025563e-02 -9.32518118e-03\n",
      " -1.16838068e-02  2.81077706e-02 -6.98414997e-03 -1.41547889e-02\n",
      " -1.09200342e-02  4.29674432e-02 -7.54409361e-02 -3.56010739e-02\n",
      " -6.79747146e-02  4.17831428e-02  5.42666575e-03 -3.34963117e-02\n",
      "  4.93628241e-03  2.88252704e-02 -7.11200168e-02 -7.47287529e-03\n",
      " -6.39631534e-03 -4.25213971e-02 -1.51117962e-02  4.46268719e-02\n",
      " -4.14154734e-02  5.08785467e-02 -1.61485026e-02 -6.56116011e-02\n",
      " -3.59388068e-02 -1.97578441e-02  1.20466664e-02 -1.49135946e-03\n",
      "  6.05632244e-02 -1.21956317e-02  1.27369310e-02  1.59314802e-02\n",
      " -1.59602659e-02 -2.09821283e-02  5.40926940e-02  6.45279757e-02\n",
      "  1.37365051e-02  9.36602421e-05 -5.15780580e-02 -4.01568257e-02\n",
      " -2.86713246e-02 -1.26602037e-02  5.47937828e-02 -3.63829116e-02\n",
      " -1.67386671e-02  4.29689913e-02  3.01295210e-02  4.16165319e-02\n",
      "  2.84094593e-02  3.53529289e-03  6.47894076e-02  7.36086835e-02] \n",
      "\n",
      "biases after updating =  [199.98759709 200.04903631 200.00950665 199.98945967 199.99595963\n",
      " 200.01451916 200.00803997 199.96865959 199.96910661 199.97681159\n",
      " 199.96901507 199.98314847 200.03190735 199.9263238  199.96733671\n",
      " 200.03876285 200.0368503  199.99906592 199.94584563 200.06338458\n",
      " 200.02649959 199.93427879 200.1185705  199.9495396  200.00778115\n",
      " 199.93643518 199.9410852  200.02089331 199.95609636 200.07363651\n",
      " 200.03006656 199.94421148 200.01188405 200.03633428 199.92630757\n",
      " 200.03896108 200.0427494  199.98324506 199.99864904 200.02734418\n",
      " 199.94889258 200.02041188 199.96750662 200.10716159 199.96777179\n",
      " 200.04845222 199.98542207 199.94499384 200.05443838 199.95666016\n",
      " 199.94855024 199.99327159 199.99287583 200.05531907 200.01907052\n",
      " 200.10359454 199.98944864 200.03387702 199.96447634 200.1230081\n",
      " 199.95652537 200.02982385 200.01945466 199.98179698 200.00475096\n",
      " 200.00353313 200.04847303 200.02527469 199.92540803 199.89200327\n",
      " 199.99305985 199.94770039 200.00184388 199.99357781 199.94580432\n",
      " 199.93342238 200.0079318  199.99820938 199.9796935  200.02400961\n",
      " 199.99111262 200.02327104 200.03284434 199.98207792 200.02945233\n",
      " 199.97278293 200.04297172 200.01733449 199.93818896 200.00111014\n",
      " 199.92783381 200.00662816 199.9939764  199.90990275 200.02517794\n",
      " 200.02399691 200.01223062 199.95293808 199.93538378 200.01294745\n",
      " 200.01393346 199.93501922 200.04844321 200.01834556 200.01480911\n",
      " 199.93309396 200.00215542 200.00715623 199.92527334 199.92223176\n",
      " 200.05719879 200.05001176 199.97862964 199.95019741 200.01660159\n",
      " 199.94498626 200.04020542 199.99403981 199.95747127 199.95731189\n",
      " 200.05143121 200.02681958 199.96372106 200.0030771  199.95058004\n",
      " 199.96996303 199.96315456 199.98882801 200.05015822 199.93514624\n",
      " 200.01757427 199.9576533  200.0065654  199.98710852 200.13553685\n",
      " 199.99603863 199.9245059  200.05738032 199.95017057 199.97004663\n",
      " 199.98846304 200.01651083 199.98588127 200.00210572 199.94353869\n",
      " 199.9901324  200.0410798  200.00608945 199.99245841 200.03791861\n",
      " 199.98339297 199.95259431 199.99489456 199.94837302 199.98582026\n",
      " 199.99840969 200.0241843  200.015968   199.98999941 200.01353857\n",
      " 200.05900292 199.91348193 200.05021513 200.05286636 199.97297957\n",
      " 199.98276652 199.96514168 200.04689843 199.99807435 200.0567201\n",
      " 200.0254092  199.98376086 200.04915385 200.0197015  199.91961442\n",
      " 200.04672862 200.06084445 200.00840164 199.98452221 200.08749921\n",
      " 199.95279207 200.04647131 199.97450556 200.03097211 200.05448919\n",
      " 200.02676338 199.92309613 199.97072186 199.95420728 200.00916246\n",
      " 199.98475164 199.96091658 200.03729132 199.90409747 200.06634748\n",
      " 199.91742906 200.0162233  199.99979041 200.06544101 199.96495267\n",
      " 199.9649703  199.95407111 199.97696959 199.94513913 200.01848374\n",
      " 199.98496893 200.02069676 199.99493204 199.99999404 200.06308624\n",
      " 200.02829834 200.01123576 199.95699881 199.99886437 200.01595086\n",
      " 200.04303212 200.02461608 199.99536415 200.01029362 199.97518298\n",
      " 200.06068098 199.98976908 200.11129667 199.95955964 199.9820596\n",
      " 200.05087591 200.04371929 199.960146   199.97097499 199.98204082\n",
      " 199.98759763 199.96406373 199.94572924 200.02501749 199.98700792\n",
      " 199.94015306 200.00899411 200.01575382 199.89341144 199.97765857\n",
      " 199.97694212 200.01089321 200.05019552 199.97219044 199.98487238\n",
      " 199.99999168 200.14444299 199.94778908 199.90449821 200.07007871\n",
      " 199.90453175 200.02743212 200.0588523  200.01410641 200.00122845\n",
      " 200.02264198 199.91222828 199.97675136 200.02554789 199.97550609\n",
      " 199.93935546 200.10144978 199.97888314 199.93975263 200.02692902\n",
      " 200.00217833 199.97462983 200.02689582 199.98277759 199.99106004\n",
      " 200.0025206  199.99770574 199.96468532 200.02107981 199.97919763\n",
      " 200.02211018 200.03147224 199.96050733 200.09388193 200.01378984\n",
      " 199.9940919  199.98398293 199.95948289 200.05016044 199.95438726\n",
      " 199.99628966 200.02763784 199.93655996 200.09311367 200.06372024\n",
      " 200.0015905  199.9460596  199.95545075 199.98054915 199.9768938\n",
      " 199.96432175 200.04452018 199.93135568 200.07182546 199.98860813\n",
      " 200.04033139 200.01029643 200.01770292 199.99786888 200.11714858\n",
      " 199.98647355 199.96918543 199.91536843 199.99816899 199.93631286\n",
      " 199.91924125 199.97417417 199.95101657 200.02766761 200.04241802\n",
      " 199.88690573 200.01616663 199.98775996 199.94214311 200.02317468\n",
      " 200.03742083 200.02123996 200.00055962 199.90819974 200.05393119\n",
      " 200.07653309 200.07808287 199.94558118 199.91238857 199.95894543\n",
      " 200.00176542 200.00919111 199.99466255 200.01987563 200.04458916\n",
      " 200.00187254 199.99649305 200.06018567 199.99310345 200.05045182\n",
      " 200.04928642 199.9281787  199.97183644 200.00090847 200.00265133\n",
      " 200.06446305 200.02452    200.01820103 199.96037811 199.95080919\n",
      " 199.93699032 200.05010906 200.0362788  200.07554207 199.95830404\n",
      " 200.08949781 199.90580987 200.02965354 199.98582619 200.10948482\n",
      " 200.03520446 199.98054927 199.97524794 200.10081807 200.01982865\n",
      " 200.03255971 199.99109033 199.98704901 200.01758906 199.97931667\n",
      " 200.01831303 200.03264981 199.98794101 199.99820043 199.94558578\n",
      " 199.96661719 200.02099004 200.01120267 199.96847754 199.96322487\n",
      " 199.94228113 199.98900594 200.07066672 199.98645528 200.0108815\n",
      " 199.96070966 200.04634902 199.92011175 199.99761962 200.03360038\n",
      " 200.01250977 200.03362843 200.09844839 200.03247459 199.94962877\n",
      " 199.98636259 199.98558398 200.05518577 199.95677521 199.90380046\n",
      " 200.02081537 199.96425907 199.97097143 200.03623983 199.92570595\n",
      " 199.95236435 199.96023806 200.04165065 200.00312938 199.95563819\n",
      " 199.97723572 200.00432624 199.9835596  199.93627866 200.01709598\n",
      " 200.08656605 200.0267732  199.99265479 200.01803694 200.0425099\n",
      " 199.92222252 200.00841039 199.97960551 200.02287611 200.02395357\n",
      " 199.96437689 199.96228599 199.9672418  199.97147445 199.93151472\n",
      " 199.97365445 200.03986398 199.92665231 199.98455134 199.99235084\n",
      " 200.01122397 199.96341252 199.9639676  200.06802113 200.02427651\n",
      " 199.99153395 200.00014643 199.99166113 199.97927046 199.91769605\n",
      " 200.0044644  200.04733634 200.05565422 200.04087506 200.02425883\n",
      " 200.08825223 199.93871905 199.97422661 200.02302886 199.96764921\n",
      " 200.01905179 200.05126026 199.92528446 199.97539744 199.99067482\n",
      " 199.98831619 200.02810777 199.99301585 199.98584521 199.98907997\n",
      " 200.04296744 199.92455906 199.96439893 199.93202529 200.04178314\n",
      " 200.00542667 199.96650369 200.00493628 200.02882527 199.92887998\n",
      " 199.99252712 199.99360368 199.9574786  199.9848882  200.04462687\n",
      " 199.95858453 200.05087855 199.9838515  199.9343884  199.96406119\n",
      " 199.98024216 200.01204667 199.99850864 200.06056322 199.98780437\n",
      " 200.01273693 200.01593148 199.98403973 199.97901787 200.05409269\n",
      " 200.06452798 200.01373651 200.00009366 199.94842194 199.95984317\n",
      " 199.97132868 199.9873398  200.05479378 199.96361709 199.98326133\n",
      " 200.04296899 200.03012952 200.04161653 200.02840946 200.00353529\n",
      " 200.06478941 200.07360868] \n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "weights before updating =  [[ 0.58319372 -0.3256563   0.06131835 ... -0.00392798 -0.08173303\n",
      "  -0.31909334]\n",
      " [-0.12246891  0.52307486  0.31760845 ...  0.04578642 -0.11179968\n",
      "  -0.41697181]\n",
      " [ 0.05903246 -0.01971662  0.34289406 ...  0.55192822  0.30842621\n",
      "  -0.13331429]\n",
      " ...\n",
      " [-0.34014162 -0.19098101  0.02564203 ...  0.37193753  0.12250902\n",
      "  -0.48780534]\n",
      " [ 0.10057049 -0.16939773  0.12759292 ... -0.22563125  0.15232662\n",
      "   0.22940699]\n",
      " [ 0.36142767 -0.09591226 -0.40998201 ...  0.32975515  0.20301156\n",
      "  -0.1257717 ]] \n",
      "\n",
      "weights after updating =  [[ 58.31937238 -32.56562964   6.13183544 ...  -0.39279813  -8.17330291\n",
      "  -31.90933382]\n",
      " [-12.24689064  52.30748551  31.76084546 ...   4.57864249 -11.17996758\n",
      "  -41.69718134]\n",
      " [  5.90324624  -1.97166246  34.28940621 ...  55.19282156  30.8426207\n",
      "  -13.33142912]\n",
      " ...\n",
      " [-34.01416236 -19.09810104   2.56420321 ...  37.193753    12.25090219\n",
      "  -48.78053448]\n",
      " [ 10.05704896 -16.93977273  12.75929156 ... -22.56312495  15.23266187\n",
      "   22.94069865]\n",
      " [ 36.1427667   -9.59122618 -40.99820067 ...  32.97551544  20.30115551\n",
      "  -12.5771703 ]] \n",
      "\n",
      "biases before updating =  [ 0.17219428 -0.05538506  0.115391   ... -0.02524565  0.20140529\n",
      " -0.21361558] \n",
      "\n",
      "biases after updating =  [200.17219428 199.94461494 200.115391   ... 199.97475435 200.20140529\n",
      " 199.78638442] \n",
      "\n",
      "\n",
      "==============================\n",
      "\n",
      "no weights and biases update for input layer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533504"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
