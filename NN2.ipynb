{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do ?\n",
    "- single dimension input\n",
    "- Model class implemented\n",
    "- Model class method `compile` implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = np.random.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40346088, -1.84182672,  0.63988283,  0.77701001,  0.05279381])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input:\n",
    "    \n",
    "    '''\n",
    "    making it for only one dimensional input\n",
    "    '''\n",
    "    def __init__(self, input_shape:int, layer_name:str):\n",
    "        \n",
    "        # self.input_shape = input_shape      \n",
    "        self.layer_name = layer_name\n",
    "        self.units = input_shape\n",
    "        self.input_data:np.array = np.zeros(self.units) # None only at time of initialization\n",
    "        \n",
    "        # these parameters doesnt exist for input layer\n",
    "        self.previous_layer = None\n",
    "        self.previous_layer_units = None\n",
    "        \n",
    "        \n",
    "    def forward_pass(self):\n",
    "        print(\"at layer      >>>>>>>>>>     \",self.layer_name)\n",
    "        return self.input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \n",
    "    def __init__(self, units:int, activation:str, layer_name:str):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.layer_name = layer_name\n",
    "        self.previous_layer:Dense = None\n",
    "        self.previous_layer_values = None\n",
    "        self.previous_layer_units = None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward_pass(self) -> np.array:\n",
    "        \n",
    "        '''\n",
    "        jab ek layer k liye forward pass chalega toh \n",
    "        wo previous layer ki values ko fetch karega and \n",
    "        calculation krke apni nayi values send karega\n",
    "        '''\n",
    "        #  determining previous layer values\n",
    "        \n",
    "        self.previous_layer_units = self.previous_layer.units\n",
    "        \n",
    "        \n",
    "        if isinstance(self.previous_layer, Input):\n",
    "            self.previous_layer_values = self.previous_layer.input_data\n",
    "\n",
    "        \n",
    "        \n",
    "        if isinstance(self.previous_layer, Dense):\n",
    "            self.previous_layer_values = self.previous_layer.activation_output\n",
    "\n",
    "    \n",
    "        \n",
    "                \n",
    "        # initializing weights and biases of this layer\n",
    "        self.__weights, self.__biases = self.__initialize_weights_and_biases(self.previous_layer_units)\n",
    "        \n",
    "        \n",
    "        # calculating output ---> WiXi + Bo \n",
    "        self.output = np.add(np.dot(self.__weights, self.previous_layer_values), self.__biases)\n",
    "\n",
    "        \n",
    "        if self.activation.lower() == 'relu':\n",
    "            activation_func = np.vectorize(self.__relu_activation)\n",
    "            self.activation_output = activation_func(self.output).astype(np.float32)\n",
    "            \n",
    "        elif self.activation.lower() == 'sigmoid':\n",
    "            activation_func = np.vectorize(self.__sigmoid_activation)\n",
    "            self.activation_output = activation_func(self.output).astype(np.float32)\n",
    "            \n",
    "        else: # linear\n",
    "            self.activation_output = self.output\n",
    "            \n",
    "        \n",
    "        print(\"\\n\\ncurrent_layer_units = \",self.units)\n",
    "        print(\"biases_shape = \",self.__biases.shape)\n",
    "        print(\"weights_shape = \",self.__weights.shape)\n",
    "        print(\"previous layer units = \",self.previous_layer_units)\n",
    "        print(\"previous layer values = \",self.previous_layer_values)\n",
    "        print(\"Output for next layer is =\", self.activation_output)\n",
    "        \n",
    "        \n",
    "        return self.activation_output\n",
    "\n",
    "        \n",
    "    def __initialize_weights_and_biases(self,previous_layer_units:int):\n",
    "        \n",
    "        '''\n",
    "        Note: using `he normal` initialization\n",
    "        '''\n",
    "        \n",
    "        biases = np.random.randn(self.units)*np.sqrt(2/previous_layer_units)\n",
    "        weights = np.random.randn(self.units, previous_layer_units)*np.sqrt(2/previous_layer_units)\n",
    "        return weights, biases\n",
    "        \n",
    "\n",
    "    def __relu_activation(self, layer_output:np.array):\n",
    "        return max(0, layer_output)\n",
    "    \n",
    "    \n",
    "    def __sigmoid_activation(self, layer_output:np.array):\n",
    "        if layer_output>0:\n",
    "            return 1/(1+np.exp(-layer_output)) # 1/(1+e^-z)\n",
    "        else:\n",
    "            return np.exp(layer_output)/(1+np.exp(layer_output)) # e^z / (1+e^z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.count = 1\n",
    "        \n",
    "               \n",
    "    def add(self, layer:Dense):\n",
    "        layer.previous_layer = None\n",
    "        '''\n",
    "        set last item in list as previous layer\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "        # if self.count>1:\n",
    "        #     layer.previous_layer = self.layers[-1]\n",
    "            \n",
    "            \n",
    "        # if isinstance(layer, (Dense, Input)):\n",
    "        #     self.layers.append(layer) # storing the layer object\n",
    "        #     self.count += 1\n",
    "        \n",
    "        # return self.layers\n",
    "    def show_layers(self):\n",
    "        print(\"=================== LAYER INFO ===================\")\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            print(f\"\"\"\n",
    "                  Index::{index}\n",
    "                        Layer Name:: {layer.layer_name} {type(layer.layer_name)}\n",
    "                        Layer Address:: {layer}  {type(layer)}      \n",
    "                        Layer units::{layer.units} {type(layer.units)}\n",
    "                        Previous Layer::{layer.previous_layer} {type(layer.previous_layer)}\n",
    "                        \n",
    "                        \"\"\")\n",
    "                \n",
    "            \n",
    "    def compile_(self, input_data:np.array): \n",
    "        '''\n",
    "        connecting all the layers\n",
    "        \n",
    "        running forward pass for all layers\n",
    "        \n",
    "        and connect input layer with input data\n",
    "        '''       \n",
    "        \n",
    "        #  connecting all the layers\n",
    "        \n",
    "        for i in range(len(self.layers)-1):\n",
    "            print(f\"set {self.layers[i].layer_name}::{self.layers[i]} as previous layer of {self.layers[i+1].layer_name}::{self.layers[i+1]}\")\n",
    "            self.layers[i+1].previous_layer = self.layers[i]\n",
    "\n",
    "        print(\"\\n\\nDONE WITH CONNECTING ALL THE LAYERS \\n\\n\")\n",
    "        \n",
    "        if isinstance(self.layers[0], Input) and self.layers[0].units == input_data.shape[0]:\n",
    "            self.layers[0].input_data = input_data\n",
    "            print(\"done with input layer!!\")\n",
    "        # else:\n",
    "            # raise ValueError(\"Input layer shape ({}) is not same as input data shape({})\".format(self.layers[0].input_shape, input_data.shape))\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            layer.forward_pass()\n",
    "            print(f\"done forward pass for {layer.layer_name}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Input(input_shape=5, layer_name=\"input_\"))\n",
    "model.add(Dense(units = 10,activation = \"sigmoid\", layer_name=\"first\"))\n",
    "model.add(Dense(units = 20,activation = \"sigmoid\", layer_name=\"second\"))\n",
    "model.add(Dense(units = 1,activation = \"sigmoid\", layer_name=\"third\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set input_::<__main__.Input object at 0x73838022be20> as previous layer of first::<__main__.Dense object at 0x73838022a200>\n",
      "set first::<__main__.Dense object at 0x73838022a200> as previous layer of second::<__main__.Dense object at 0x738380228d60>\n",
      "set second::<__main__.Dense object at 0x738380228d60> as previous layer of third::<__main__.Dense object at 0x73838022bb80>\n",
      "\n",
      "\n",
      "DONE WITH CONNECTING ALL THE LAYERS \n",
      "\n",
      "\n",
      "done with input layer!!\n",
      "at layer      >>>>>>>>>>      input_\n",
      "done forward pass for input_\n",
      "\n",
      "\n",
      "current_layer_units =  10\n",
      "biases_shape =  (10,)\n",
      "weights_shape =  (10, 5)\n",
      "previous layer units =  5\n",
      "previous layer values =  [-1.40346088 -1.84182672  0.63988283  0.77701001  0.05279381]\n",
      "Output for next layer is = [0.8049293  0.04211444 0.74088365 0.43989772 0.04905093 0.51171005\n",
      " 0.8193818  0.7061816  0.7506211  0.08681619]\n",
      "done forward pass for first\n",
      "\n",
      "\n",
      "current_layer_units =  20\n",
      "biases_shape =  (20,)\n",
      "weights_shape =  (20, 10)\n",
      "previous layer units =  10\n",
      "previous layer values =  [0.8049293  0.04211444 0.74088365 0.43989772 0.04905093 0.51171005\n",
      " 0.8193818  0.7061816  0.7506211  0.08681619]\n",
      "Output for next layer is = [0.6159665  0.47549567 0.22655134 0.30833548 0.42075184 0.7020477\n",
      " 0.8756591  0.46353787 0.70297897 0.41700011 0.3875427  0.4629477\n",
      " 0.15817456 0.7935877  0.83027345 0.6598636  0.7020667  0.7898411\n",
      " 0.7114864  0.6175001 ]\n",
      "done forward pass for second\n",
      "\n",
      "\n",
      "current_layer_units =  1\n",
      "biases_shape =  (1,)\n",
      "weights_shape =  (1, 20)\n",
      "previous layer units =  20\n",
      "previous layer values =  [0.6159665  0.47549567 0.22655134 0.30833548 0.42075184 0.7020477\n",
      " 0.8756591  0.46353787 0.70297897 0.41700011 0.3875427  0.4629477\n",
      " 0.15817456 0.7935877  0.83027345 0.6598636  0.7020667  0.7898411\n",
      " 0.7114864  0.6175001 ]\n",
      "Output for next layer is = [0.6978122]\n",
      "done forward pass for third\n"
     ]
    }
   ],
   "source": [
    "model.compile_(input_)\n",
    "# check dtype of input object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40346088, -1.84182672,  0.63988283,  0.77701001,  0.05279381])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== LAYER INFO ===================\n",
      "\n",
      "                  Index::0\n",
      "                        Layer Name:: input_ <class 'str'>\n",
      "                        Layer Address:: <__main__.Input object at 0x73838022be20>  <class '__main__.Input'>      \n",
      "                        Layer units::5 <class 'int'>\n",
      "                        Previous Layer::None <class 'NoneType'>\n",
      "                        \n",
      "                        \n",
      "\n",
      "                  Index::1\n",
      "                        Layer Name:: first <class 'str'>\n",
      "                        Layer Address:: <__main__.Dense object at 0x73838022a200>  <class '__main__.Dense'>      \n",
      "                        Layer units::10 <class 'int'>\n",
      "                        Previous Layer::<__main__.Input object at 0x73838022be20> <class '__main__.Input'>\n",
      "                        \n",
      "                        \n",
      "\n",
      "                  Index::2\n",
      "                        Layer Name:: second <class 'str'>\n",
      "                        Layer Address:: <__main__.Dense object at 0x738380228d60>  <class '__main__.Dense'>      \n",
      "                        Layer units::20 <class 'int'>\n",
      "                        Previous Layer::<__main__.Dense object at 0x73838022a200> <class '__main__.Dense'>\n",
      "                        \n",
      "                        \n",
      "\n",
      "                  Index::3\n",
      "                        Layer Name:: third <class 'str'>\n",
      "                        Layer Address:: <__main__.Dense object at 0x73838022bb80>  <class '__main__.Dense'>      \n",
      "                        Layer units::1 <class 'int'>\n",
      "                        Previous Layer::<__main__.Dense object at 0x738380228d60> <class '__main__.Dense'>\n",
      "                        \n",
      "                        \n"
     ]
    }
   ],
   "source": [
    "model.show_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40346088, -1.84182672,  0.63988283,  0.77701001,  0.05279381])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
